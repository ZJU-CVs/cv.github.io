---
layout:     post
title:      对抗样本对人工智能应用的威胁
subtitle:   
date:       2020-06-30
author:     JY
header-img: img/post-bg.jpg
catalog: true
tags:
    - Notes
---



#### 1. 背景介绍

> - 研究者已发现在某些人工智能模型的输入上增加少量“干扰”，可在人类无法分辨区别的情况下使模型对输入的分类结果做出错误判断。这种经过处理后的输入样本通常被称为**对抗样本**。
>
> - 寻找对抗样本可以归纳为下述问题：
>
>   > 针对模型$F:x \rightarrow y$，其中x为模型的输入，y为模型的输出。对于原输入x，找到输入x'，使得模型的输出被最大化为目标标签y'，y'不等于y，且要求x与 x'的“距离”最小，此时**训练的目标不再是模型的优化而是生成符合条件的细微扰动**
>   
>   举例：
>   
>   >  输入一张车辆头部的影像，训练好的inceptionV3模型可以很好地识别出图片的所属分类为“sports car”。然后设计合适的反馈机制并逐步训练得到了一张人类看起来与原图并无区别的车辆头部影像，再次输入给inceptionV3模型，模型给出的识别结果却成了“mountain bike”。
>   >
>   > ![img](https://github.com/ZJU-CVs/zju-cvs.github.io/raw/master/img/notes/adversarial-sample/1.png )
>   
>   
>   
>   ![img](https://www.aqniu.com/wp-content/uploads/2019/01/22.png)
>   
>   图2