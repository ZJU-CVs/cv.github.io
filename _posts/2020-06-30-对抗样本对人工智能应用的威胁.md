---
layout:     post
title:      对抗样本对人工智能应用的威胁及防护
subtitle:   
date:       2020-06-30
author:     JY
header-img: img/post-bg.jpg
catalog: true
tags:
    - Notes
---



#### 1. 背景介绍

> - 研究者已发现在某些人工智能模型的输入上增加少量“干扰”，可在人类无法分辨区别的情况下使模型对输入的分类结果做出错误判断。这种经过处理后的输入样本通常被称为**对抗样本**。
>
> - 寻找对抗样本可以归纳为下述问题：
>
>   > 针对模型$F:x \rightarrow y$，其中x为模型的输入，y为模型的输出。对于原输入x，找到输入x'，使得模型的输出被最大化为目标标签y'，y'不等于y，且要求x与 x'的“距离”最小，此时**训练的目标不再是模型的优化而是生成符合条件的细微扰动**
>   
>   举例：
>   
>   >  输入一张车辆头部的影像，训练好的inceptionV3模型可以很好地识别出图片的所属分类为“sports car”。然后设计合适的反馈机制并逐步训练得到了一张人类看起来与原图并无区别的车辆头部影像，再次输入给inceptionV3模型，模型给出的识别结果却成了“mountain bike”。
>   >
>   > <img src="https://github.com/ZJU-CVs/zju-cvs.github.io/raw/master/img/notes/adversarial-sample/1.png" alt="img" style="zoom:50%;" /><img src="https://github.com/ZJU-CVs/zju-cvs.github.io/raw/master/img/notes/adversarial-sample/2.png" alt="img" style="zoom:50%;" />
>
> - 模型能够获得优质结果其实最取决于接近“无穷”量级的训练数据。因此在所有有效输入组成的整个数据搜索空间中，我们通过上述“训练”方式所能得到的模型就是一个在相对有限情况下有效的模型了。也正是因为这个局限性，给予了攻击者利用对抗样本去攻击模型的足够空间



#### 2. 对抗样本防护技术

> 一般可分为主动防护方式和被动防护方式。主动防护方式是指通过技术加固，把神经网络本身的对抗样本防护能力进行提升；而被动防护方式则是独立于神经网络，通常置于神经网络输入之前，起到针对对抗样本的防护作用。

**主动防护方式**

> 网络蒸馏：将第一个深度神经网络输出的分类可能性结果输入到第二个网络中进行训练。通过这种方式，网络蒸馏可以从原有神经网络中提取知识来增进网络的鲁棒性。通过这种技术处理，降低了模型对小扰动的敏感度从而提升了对对抗样本的抵抗能力。(《Distillationas a defense to adversarial perturbations against deep neural networks》)
>
> 



> 对抗训练
>
> 分类器鲁棒化